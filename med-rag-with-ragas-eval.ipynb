{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T03:34:57.693214Z",
     "iopub.status.busy": "2024-11-14T03:34:57.692687Z",
     "iopub.status.idle": "2024-11-14T03:35:14.378606Z",
     "shell.execute_reply": "2024-11-14T03:35:14.377151Z",
     "shell.execute_reply.started": "2024-11-14T03:34:57.693158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain faiss-gpu transformers evaluate ragas datasets huggingface_hub mlflow tqdm\n",
    "# !gym stable-baselines3 onnx onnxruntime\n",
    "# !pip install onnx onnxruntime\n",
    "!pip install optimum[onnxruntime] onnx\n",
    "!pip install sentence-transformers\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T03:35:14.384639Z",
     "iopub.status.busy": "2024-11-14T03:35:14.384263Z",
     "iopub.status.idle": "2024-11-14T03:35:29.735103Z",
     "shell.execute_reply": "2024-11-14T03:35:29.733339Z",
     "shell.execute_reply.started": "2024-11-14T03:35:14.384591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install packaging==23.0 mlflow==2.17.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPORTS AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "# from optimum.intel import IncQuantizer\n",
    "# from torch.ao.quantization import get_default_qconfig, quantize_dynamic\n",
    "import torch.cuda\n",
    "from tqdm import tqdm\n",
    "import mlflow.pytorch\n",
    "# import onnx\n",
    "# from onnxruntime.quantization import quantize_static, QuantizationMode, CalibrationDataReader, QuantType, QuantFormat\n",
    "# import onnxruntime as ort\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from transformers import pipeline, ElectraForQuestionAnswering, ElectraTokenizer, AlbertForQuestionAnswering, AlbertTokenizer, AutoModelForQuestionAnswering, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "import os\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "# from mlflow.llm.evaluate import evaluate_llm, evaluate_rag\n",
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.llms import HuggingFaceLLM\n",
    "from transformers import default_data_collator\n",
    "from langchain_core.load import dumpd, dumps, load, loads\n",
    "import pickle\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Optional, Sequence\n",
    "from langchain.schema import Document\n",
    "from accelerate import Accelerator\n",
    "from torch.cuda.amp import autocast\n",
    "# from langchain.pydantic_v1 import Extra, root_validator\n",
    "\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.document_transformers.embeddings_redundant_filter import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_transformers.long_context_reorder import LongContextReorder\n",
    "# from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# else:\n",
    "#     torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "# Set default device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# print('ort version: ', ort.__version__)\n",
    "\n",
    "quantize_llm = True\n",
    "eval_llm = False\n",
    "rag_chain_path = 'rag/rag_chain.pkl'\n",
    "quantized_model_path = 'quantized_model'\n",
    "quantized_tokenizer_path = 'quantized_tokenizer'\n",
    "directories = ['fine_tuned_model', quantized_model_path, quantized_tokenizer_path, 'fine_tuned_tokenizer', 'rag', 'Vectorstore/chromadb', 'fine-tune-llm-results', 'fine-tune-llm-logs']\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "experiment_id = mlflow.create_experiment('BioASQ RAG')\n",
    "mlflow.set_experiment('BioASQ RAG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-14T03:35:39.676514Z",
     "iopub.status.idle": "2024-11-14T03:35:39.677013Z",
     "shell.execute_reply": "2024-11-14T03:35:39.676797Z",
     "shell.execute_reply.started": "2024-11-14T03:35:39.676775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_bioasq_data(train_file, test_file):\n",
    "    \"\"\"Load and process BioASQ data, utilizing all fields (question, ideal_answer, articles, snippets).\"\"\"\n",
    "    \n",
    "    def build_dataset(data):\n",
    "        dataset = []\n",
    "        for item in data:\n",
    "            question = item['question']\n",
    "            ideal_answer = item['ideal_answer']\n",
    "            \n",
    "            context = \"\"\n",
    "            for article in item.get('articles', []):\n",
    "                title = article.get('title', '')  # Default to empty string if 'title' is missing\n",
    "                abstract = article.get('abstract', '')  # Default to empty string if 'abstract' is missing\n",
    "                context += f\"{title} {abstract} \"  # Concatenate safely\n",
    "            \n",
    "            for snippet in item.get('snippets', []):\n",
    "                title = snippet.get('title', '')  # Default to empty string if 'title' is missing\n",
    "                abstract = snippet.get('abstract', '')  # Default to empty string if 'abstract' is missing\n",
    "                context += f\"{title} {abstract} \"  # Concatenate safely\n",
    "            \n",
    "            context += \" \".join(item.get('concepts', []))  # Ensure concepts are strings\n",
    "            \n",
    "            # Construct train example\n",
    "            dataset.append({\n",
    "                'question': question,\n",
    "                'context': context,\n",
    "                'ideal_answer': ideal_answer\n",
    "            })\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    with open(train_file, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    with open(test_file, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    train_dataset = build_dataset(train_data)\n",
    "    test_dataset = build_dataset(test_data)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Initialize MLFlow logging for the entire pipeline\n",
    "mlflow.autolog()\n",
    "with mlflow.start_run(run_name=\"Dataset Creation\") as dataset_creation_run:\n",
    "\n",
    "    print('Dataset Creation')\n",
    "    # Load BioASQ training and testing datasets\n",
    "    train_file = '/kaggle/input/bio-asq/training12b_train.json'\n",
    "    test_file = '/kaggle/input/bio-asq/training12b_test.json'\n",
    "    train_dataset, test_dataset = load_bioasq_data(train_file, test_file)\n",
    "\n",
    "    # Optionally log dataset info to MLFlow\n",
    "    mlflow.log_param('bio_asq_train_dataset_size', len(train_dataset))\n",
    "    mlflow.log_param('bio_asq_test_dataset_size', len(test_dataset))\n",
    "\n",
    "    # Save datasets as JSON files\n",
    "    train_output_file = 'bio_asq_train_dataset.json'\n",
    "    test_output_file = 'bio_asq_test_dataset.json'\n",
    "\n",
    "    with open(train_output_file, 'w') as f:\n",
    "        json.dump(train_dataset, f, indent=4)\n",
    "\n",
    "    with open(test_output_file, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)\n",
    "\n",
    "    # Log the file paths to MLFlow for tracking\n",
    "    mlflow.log_artifact(train_output_file)\n",
    "    mlflow.log_artifact(test_output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINE TUNE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "finetuned_model_path = '/kaggle/input/bioasq-fine-tuned-t5-small/transformers/default/1'\n",
    "finetuned_tokenizer_path = '/kaggle/input/bioasq-fine-tuned-t5-small-tokenizer/transformers/default/1'\n",
    "\n",
    "quantized_finetuned_model_path = '/kaggle/input/bioasq-quantized-fine-tuned-t5-small/onnx/default/1'\n",
    "quantized_finetuned_tokenizer_path = '/kaggle/input/bioasq-quantized-fine-tuned-t5-small-tokenizer/transformers/default/1'\n",
    "\n",
    "class BioASQDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "    \n",
    "\n",
    "def fine_tune_llm(train_dataset, test_dataset, model_name='t5-small'):\n",
    "    \"\"\"Fine-tune a pre-trained LLM on BioASQ data.\"\"\"\n",
    "    \n",
    "    if os.path.exists(finetuned_model_path) and os.path.exists(finetuned_tokenizer_path):\n",
    "        print('Loading fine-tuned model and tokenizer from checkpoint files')\n",
    "        model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path)\n",
    "        tokenizer = T5Tokenizer.from_pretrained(finetuned_tokenizer_path)\n",
    "        return model, tokenizer\n",
    "    else:\n",
    "        # Load model and tokenizer from Hugging Face\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    print(f'Loaded model {model_name} and tokenizer from Hugging Face')\n",
    "\n",
    "    # Tokenize the questions and answers, include context for the input\n",
    "    train_encodings = tokenizer([f\"question: {item['question']} context: {item['context']}\" for item in train_dataset],\n",
    "                                truncation=True, padding=True, max_length=256)\n",
    "    train_labels = tokenizer([item['ideal_answer'] for item in train_dataset], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "    test_encodings = tokenizer([f\"question: {item['question']} context: {item['context']}\" for item in test_dataset],\n",
    "                               truncation=True, padding=True, max_length=256)\n",
    "    test_labels = tokenizer([item['ideal_answer'] for item in test_dataset], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "    train_data = BioASQDataset(train_encodings, train_labels)\n",
    "    test_data = BioASQDataset(test_encodings, test_labels)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='/fine-tune-llm-results',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='/fine-tune-llm-logs',\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Fine-tune the model on BioASQ dataset\n",
    "mlflow.autolog()\n",
    "with mlflow.start_run(run_name=\"Fine Tuning\") as fine_tuning_run:\n",
    "    if os.path.exists(quantized_finetuned_model_path) and os.path.exists(quantized_finetuned_tokenizer_path) and quantize_llm:\n",
    "        print('Loading quantized_fine-tuned model and tokenizer from checkpoint files')\n",
    "        llm = ORTModelForSeq2SeqLM.from_pretrained(quantized_finetuned_model_path)\n",
    "        tokenizer = T5Tokenizer.from_pretrained(quantized_finetuned_tokenizer_path)\n",
    "    else:\n",
    "        model_name = 't5-small'\n",
    "        fine_tuned_model, fine_tuned_tokenizer = fine_tune_llm(train_dataset, test_dataset, model_name)\n",
    "\n",
    "        # Save the fine-tuned model\n",
    "        fine_tuned_model.save_pretrained('fine_tuned_model')\n",
    "        fine_tuned_tokenizer.save_pretrained('fine_tuned_tokenizer')\n",
    "\n",
    "        # Log model parameters to MLFlow\n",
    "        mlflow.log_param('fine_tuned_model_name', model_name)\n",
    "        mlflow.log_param('quantize_llm', quantize_llm)\n",
    "\n",
    "        # Save model weights to MLFlow\n",
    "        torch.save(fine_tuned_model.state_dict(), 'fine_tuned_model_weights.pth')\n",
    "        mlflow.log_artifact('fine_tuned_model_weights.pth', artifact_path='fine_tuned_model')\n",
    "        \n",
    "        if quantize_llm:\n",
    "            print('Quantizing llm model')\n",
    "            llm = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "                finetuned_model_path,\n",
    "                from_transformers=True,\n",
    "                export=True,  # Exports the model to ONNX\n",
    "                # quantization_config=AutoQuantizationConfig.default()\n",
    "            )\n",
    "            tokenizer = T5Tokenizer.from_pretrained(finetuned_tokenizer_path)\n",
    "            \n",
    "            # Save the quantized model\n",
    "            llm.save_pretrained(quantized_model_path)\n",
    "            tokenizer.save_pretrained(quantized_tokenizer_path)\n",
    "        else:\n",
    "            llm = fine_tuned_model\n",
    "            tokenizer = fine_tuned_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def chunk_text(text, max_tokens, tokenizer):\n",
    "    \"\"\"Split text into chunks within the token limit.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    return [tokenizer.convert_tokens_to_string(chunk) for chunk in chunks]\n",
    "\n",
    "def generate_from_chunks(question, context, tokenizer, model, max_tokens=512):\n",
    "    \"\"\"Generate an answer by aggregating outputs from context chunks.\"\"\"\n",
    "    chunks = chunk_text(context, max_tokens - len(tokenizer.tokenize(question)) - 10, tokenizer)\n",
    "    answers = []\n",
    "    for chunk in chunks:\n",
    "        input_text = f\"question: {question} context: {chunk}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_tokens, truncation=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        # outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=150, \n",
    "            num_beams=5,  # Beam width\n",
    "            early_stopping=True, \n",
    "            length_penalty=1.0  # Control length balance\n",
    "        )\n",
    "        answers.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    return \" \".join(answers)  # Combine chunk outputs\n",
    "\n",
    "# Function to evaluate the fine-tuned model using Hugging Face's evaluator\n",
    "def evaluate_llm_with_huggingface(model, tokenizer, test_dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    question_ans = 1\n",
    "    for item in tqdm(test_dataset, desc=\"Evaluating LLM\"):\n",
    "        print(f'Answering question {question_ans}')\n",
    "        question_ans += 1\n",
    "        question = item['question']\n",
    "        context = item['context']\n",
    "        ideal_answer = item['ideal_answer']\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Move the quantized model to the same device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        \n",
    "        # Generate answer\n",
    "        with autocast():\n",
    "            pred = generate_from_chunks(question, context, tokenizer, model)\n",
    "        \n",
    "        # Store predictions and references\n",
    "        predictions.append(pred)\n",
    "        references.append(ideal_answer)\n",
    "    \n",
    "    # Evaluate using ROUGE\n",
    "    result = rouge.compute(predictions=predictions, references=references)\n",
    "    for key, value in result.items():\n",
    "        mlflow.log_metric(key, value)\n",
    "    print(\"ROUGE scores:\", result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "mlflow.autolog()\n",
    "with mlflow.start_run(run_name=\"LLM Evaluation\", parent_run_id=fine_tuning_run.info.run_id) as llm_evaluation_run:\n",
    "    mlflow.log_param('eval_llm', eval_llm)\n",
    "    if eval_llm:\n",
    "        llm_metrics = evaluate_llm_with_huggingface(llm, tokenizer, test_dataset)\n",
    "        print(f\"LLM Evaluation Metrics: {llm_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE RAG DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_rag_dataset(train_data, test_data):\n",
    "    \"\"\"Create a dataset for RAG from BioASQ data.\"\"\"\n",
    "    \n",
    "    def build_dataset(data):\n",
    "        dataset = []\n",
    "\n",
    "        # Create text splitter for RAG\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 1250,\n",
    "            chunk_overlap = 100,\n",
    "            length_function = len,\n",
    "            is_separator_regex = False\n",
    "        )\n",
    "        \n",
    "        for item in data:\n",
    "            chunks = text_splitter.split_text(item['context'])\n",
    "            \n",
    "            # Add each chunk as a separate document with metadata\n",
    "            for chunk in chunks:\n",
    "                dataset.append({\n",
    "                    'context': chunk,\n",
    "                    'metadata': {\n",
    "                        'question': item['question'],\n",
    "                        'ideal_answer': item['ideal_answer']\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    train_dataset = build_dataset(train_data)\n",
    "    test_dataset = build_dataset(test_data)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Initialize MLFlow logging for the entire pipeline\n",
    "mlflow.autolog()\n",
    "with mlflow.start_run(run_name=\"Dataset Creation for RAG\", parent_run_id=llm_evaluation_run.info.run_id) as rag_dataset_creation_run:\n",
    "\n",
    "    train_dataset_rag, test_dataset_rag = load_bioasq_data(train_dataset, test_dataset)\n",
    "\n",
    "    # Save datasets as JSON files\n",
    "    train_output_file = 'bio_asq_train_dataset_rag.json'\n",
    "    test_output_file = 'bio_asq_test_dataset_rag.json'\n",
    "\n",
    "    train_string = dumps(train_dataset_rag, pretty=True)\n",
    "    test_string = dumps(test_dataset_rag, pretty=True)\n",
    "\n",
    "    with open(train_output_file, 'w') as f:\n",
    "        json.dump(train_string, f, indent=4)\n",
    "\n",
    "    with open(test_output_file, 'w') as f:\n",
    "        json.dump(test_string, f, indent=4)\n",
    "\n",
    "    # Log the file paths to MLFlow for tracking\n",
    "    mlflow.log_artifact(train_output_file)\n",
    "    mlflow.log_artifact(test_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE RAG PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class BgeRerank(BaseDocumentCompressor):\n",
    "    model_name:str = 'BAAI/bge-reranker-v2-m3'\n",
    "    \"\"\"Model name to use for reranking.\"\"\"\n",
    "    top_n: int = 3\n",
    "    \"\"\"Number of documents to return.\"\"\"\n",
    "    model:CrossEncoder = CrossEncoder(model_name)\n",
    "    \"\"\"CrossEncoder instance to use for reranking.\"\"\"\n",
    "\n",
    "    def bge_rerank(self, query,docs):\n",
    "        model_inputs =  [[query, doc] for doc in docs]\n",
    "        scores = self.model.predict(model_inputs)\n",
    "        results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        return results[:self.top_n]\n",
    "\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        callbacks: Optional[Callbacks] = None,\n",
    "    ) -> Sequence[Document]:\n",
    "        \"\"\"\n",
    "        Compress documents using BAAI/bge-reranker models.\n",
    "\n",
    "        Args:\n",
    "            documents: A sequence of documents to compress.\n",
    "            query: The query to use for compressing the documents.\n",
    "            callbacks: Callbacks to run during the compression process.\n",
    "\n",
    "        Returns:\n",
    "            A sequence of compressed documents.\n",
    "        \"\"\"\n",
    "        if len(documents) == 0:  # to avoid empty api call\n",
    "            return []\n",
    "        doc_list = list(documents)\n",
    "        _docs = [d.page_content for d in doc_list]\n",
    "        results = self.bge_rerank(query, _docs)\n",
    "        final_results = []\n",
    "        for r in results:\n",
    "            doc = doc_list[r[0]]\n",
    "            doc.metadata[\"relevance_score\"] = r[1]\n",
    "            final_results.append(doc)\n",
    "        return final_results\n",
    "    \n",
    "def create_rag_pipeline(train_dataset_rag):\n",
    "    \"\"\"Create a RAGChain pipeline using the fine-tuned model and ChromaDB.\"\"\"\n",
    "\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Create Chroma index\n",
    "    chroma_dir = 'Vectorstore/chromadb'\n",
    "    vectorstore = Chroma(embedding_function = embedding_model,\n",
    "                         persist_directory = chroma_dir,\n",
    "                     collection_name = \"bioasq_train_documents\")\n",
    "\n",
    "    vectorstore.add_documents(train_dataset_rag)\n",
    "    vectorstore.persist()\n",
    "\n",
    "    mlflow.log_param('rag_vectorstore', vectorstore)\n",
    "    mlflow.log_artifact(chroma_dir)\n",
    "\n",
    "    bm25_retriever = BM25Retriever.from_documents(train_dataset_rag)\n",
    "    bm25_retriever.k=10    \n",
    "\n",
    "    vs_retriever = vectorstore.as_retriever(search_kwargs = {\"k\":10})\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(retrievers = [bm25_retriever,vs_retriever], weight = [0.5,0.5])\n",
    "\n",
    "    redundant_filter = EmbeddingsRedundantFilter(embeddings=embedding_model)\n",
    "\n",
    "    reordering = LongContextReorder()\n",
    "\n",
    "    reranker = BgeRerank()\n",
    "\n",
    "    pipeline_compressor = DocumentCompressorPipeline(transformers = [redundant_filter, reordering, reranker])\n",
    "\n",
    "    compression_pipeline = ContextualCompressionRetriever(base_compressor = pipeline_compressor, base_retriever = ensemble_retriever)\n",
    "\n",
    "    qa_advanced = RetrievalQA.from_chain_type(llm = quantized_model,\n",
    "                                    chain_type = \"stuff\",\n",
    "                                    retriever = compression_pipeline,\n",
    "                                    return_source_documents = True)\n",
    "    mlflow.log_param('rag_chain_created', True)\n",
    "    return qa_advanced\n",
    "\n",
    "def save_rag_pipeline(rag_chain, rag_chain_path):\n",
    "    \"\"\"Save the RAGChain pipeline to a file.\"\"\"\n",
    "    with open(rag_chain_path, 'wb') as f:\n",
    "        pickle.dump(rag_chain, f)\n",
    "\n",
    "    mlflow.log_artifact(rag_chain_path)\n",
    "\n",
    "mlflow.autolog()\n",
    "with mlflow.start_run(run_name=\"RAG Pipeline Creation\", parent_run_id=rag_dataset_creation_run.info.run_id) as rag_pipeline_creation_run:\n",
    "    rag_chain = create_rag_pipeline(train_dataset_rag)\n",
    "    save_rag_pipeline(rag_chain, rag_chain_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATE RAG PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "def evaluate_rag_pipeline(rag_chain, test_dataset_rag):\n",
    "    \"\"\"Evaluate the RAGChain pipeline using the test dataset.\"\"\"\n",
    "    questions = []\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    ground_truths = []\n",
    "\n",
    "    for item in tqdm(test_dataset_rag, desc=\"Evaluating RAG Pipeline\"):\n",
    "        question = item['question']\n",
    "        ideal_answer = item['ideal_answer']\n",
    "        result = rag_chain.invoke({\"query\": question})\n",
    "        questions.append(question)\n",
    "        answers.append(result['result'])\n",
    "        contexts.append([context.page_content for context in result['source_documents']])\n",
    "        ground_truths.append(ideal_answer)\n",
    "\n",
    "    response_dataset = Dataset.from_dict({\n",
    "        \"question\" : questions,\n",
    "        \"answer\" : answers,\n",
    "        \"contexts\" : contexts,\n",
    "        \"ground_truth\" : ground_truths\n",
    "    })\n",
    "\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "        answer_correctness,\n",
    "    ]\n",
    "    #\n",
    "    eval_results = evaluate(response_dataset, metrics,raise_exceptions=False)\n",
    "    \n",
    "    mlflow.log_metric('rag_pipeline_evaluation', eval_results)\n",
    "    return eval_results\n",
    "\n",
    "mlflow.autolog()\n",
    "with mlflow.start_run(run_name=\"RAG Pipeline Evaluation\", parent_run_id=rag_pipeline_creation_run.info.run_id) as rag_pipeline_evaluation_run:\n",
    "    eval_results = evaluate_rag_pipeline(rag_chain, test_dataset_rag)\n",
    "    print(f\"RAG Pipeline Evaluation Metrics: {eval_results}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6081719,
     "sourceId": 9900539,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
