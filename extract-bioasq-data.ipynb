{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bde5cb7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-13T16:37:48.268032Z",
     "iopub.status.busy": "2024-11-13T16:37:48.267536Z",
     "iopub.status.idle": "2024-11-13T16:51:28.576880Z",
     "shell.execute_reply": "2024-11-13T16:51:28.575336Z"
    },
    "papermill": {
     "duration": 820.31788,
     "end_time": "2024-11-13T16:51:28.580281",
     "exception": false,
     "start_time": "2024-11-13T16:37:48.262401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 85/85 [03:26<00:00,  2.43s/question, batch=85]\n",
      "Processing questions: 100%|██████████| 85/85 [03:19<00:00,  2.35s/question, batch=85]\n",
      "Processing questions: 100%|██████████| 85/85 [03:33<00:00,  2.51s/question, batch=85]\n",
      "Processing questions: 100%|██████████| 85/85 [03:20<00:00,  2.36s/question, batch=85]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from requests.adapters import HTTPAdapter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Function to create a retry session with exponential backoff\n",
    "def create_retry_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504, 429])\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    return session\n",
    "\n",
    "# Function to fetch article metadata for a list of PMIDs in batch\n",
    "def fetch_pubmed_articles_batch(pmids):\n",
    "    efetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    \n",
    "    # Prepare parameters for batch request (up to 100-200 PMIDs per request)\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": \",\".join(pmids),  # Join the PMIDs in a comma-separated string\n",
    "        \"retmode\": \"xml\",\n",
    "        \"retmax\": str(len(pmids))  # Number of results to return, should be the size of the batch\n",
    "    }\n",
    "    \n",
    "    session = create_retry_session()\n",
    "    \n",
    "    try:\n",
    "        # Send the batch request\n",
    "        response = session.get(efetch_url, params=params)\n",
    "        response.raise_for_status()  # Will raise an exception for HTTP 4xx/5xx responses\n",
    "        \n",
    "        # Parse the XML response\n",
    "        tree = ET.ElementTree(ET.fromstring(response.content))\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Parse article data from the XML\n",
    "        articles = []\n",
    "        for article in root.findall(\".//PubmedArticle\"):\n",
    "            title = article.find(\".//ArticleTitle\").text if article.find(\".//ArticleTitle\") is not None else \"No Title\"\n",
    "            abstract = article.find(\".//Abstract/AbstractText\").text if article.find(\".//Abstract/AbstractText\") is not None else \"No Abstract\"\n",
    "            authors = article.findall(\".//AuthorList/Author\")\n",
    "            author_names = [\n",
    "                f\"{author.find('.//LastName').text} {author.find('.//ForeName').text}\"\n",
    "                for author in authors\n",
    "                if author.find('.//LastName') is not None and author.find('.//ForeName') is not None\n",
    "            ]\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"authors\": author_names\n",
    "            })\n",
    "        return articles\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching batch of PMIDs: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to fetch concept descriptions (if available)\n",
    "def fetch_concept_description(concept_url):\n",
    "    try:\n",
    "        if \"disease-ontology\" in concept_url:\n",
    "            # Fetching disease ontology metadata (dummy response, needs proper API call)\n",
    "            response = requests.get(concept_url)\n",
    "            if response.status_code == 200:\n",
    "                return response.json().get(\"label\", \"No description available\")\n",
    "        elif \"mesh\" in concept_url:\n",
    "            return \"MeSH term description for \" + concept_url.split(\"=\")[-1]  # Placeholder text\n",
    "        return \"No description available\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"Error fetching concept description\"\n",
    "\n",
    "# Function to process the BioASQ question and extract article information\n",
    "def process_bioasq_question(question):\n",
    "    documents = question[\"documents\"]\n",
    "    concepts = question.get(\"concepts\", [])\n",
    "    snippets = question[\"snippets\"]\n",
    "    pmids = [get_pmid_from_url(url) for url in documents]\n",
    "    \n",
    "    result = {\n",
    "        \"question\": question[\"body\"],\n",
    "        \"ideal_answer\": \" \".join(question[\"ideal_answer\"]) if \"ideal_answer\" in question else \"\",\n",
    "        \"exact_answer\": question.get(\"exact_answer\", []),\n",
    "        \"articles\": [],\n",
    "        \"concepts\": [],  # Empty list for concepts if they are missing\n",
    "        \"snippets\": [],\n",
    "        \"snippet_texts\": [snippet[\"text\"] for snippet in snippets]\n",
    "    }\n",
    "    \n",
    "    # Fetch concept descriptions if available\n",
    "    if concepts:  # Only process concepts if they exist\n",
    "        for concept_url in concepts:\n",
    "            description = fetch_concept_description(concept_url)  # Fetch the textual description\n",
    "            result[\"concepts\"].append(description)\n",
    "    \n",
    "    # Process PMIDs in batches (e.g., 100 PMIDs per batch)\n",
    "    batch_size = 100\n",
    "\n",
    "    snippet_pmids = [get_pmid_from_url(snippet[\"document\"]) for snippet in snippets]\n",
    "    for i in range(0, len(snippet_pmids), batch_size):\n",
    "        batch_pmids = snippet_pmids[i:i + batch_size]\n",
    "        articles = fetch_pubmed_articles_batch(batch_pmids)\n",
    "        if articles:\n",
    "            result[\"snippets\"].extend(articles)\n",
    "    \n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        batch_pmids = pmids[i:i + batch_size]\n",
    "        articles = fetch_pubmed_articles_batch(batch_pmids)\n",
    "        if articles:\n",
    "            result[\"articles\"].extend(articles)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Helper function to extract the PMID from the PubMed URL\n",
    "def get_pmid_from_url(url):\n",
    "    # PubMed URLs look like: https://pubmed.ncbi.nlm.nih.gov/{pmid}/\n",
    "    return url.strip('/').split('/')[-1]\n",
    "\n",
    "# Function to process the entire dataset and save the output in alternating files\n",
    "def process_bioasq_data(input_data, file):\n",
    "    # Initialize the output list that will hold all questions\n",
    "    all_processed_questions = []\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Prepare output files\n",
    "    output_file_1 = f\"{file}_output_batch_1.json\"\n",
    "    output_file_2 = f\"{file}_output_batch_2.json\"\n",
    "    \n",
    "    with tqdm(input_data, desc=\"Processing questions\", unit=\"question\") as question_progress:\n",
    "        for question in question_progress:\n",
    "            result = process_bioasq_question(question)\n",
    "            all_processed_questions.append(result)\n",
    "            # Alternate between the two output files\n",
    "            output_file = output_file_1 if batch_count % 2 == 0 else output_file_2\n",
    "            \n",
    "            # Append the result to the appropriate output file\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(all_processed_questions, f, indent=4)\n",
    "            \n",
    "            batch_count += 1\n",
    "\n",
    "            # Update progress\n",
    "            question_progress.set_postfix(batch=batch_count)\n",
    "            \n",
    "            # Optionally, add a delay to respect the rate limits (1 request per second)\n",
    "            time.sleep(1)\n",
    "\n",
    "    # # Process each question in the input data\n",
    "    # for question in input_data:\n",
    "    #     result = process_bioasq_question(question)\n",
    "    #     all_processed_questions.append(result)\n",
    "    \n",
    "    # # Write the entire results in alternating files after each batch\n",
    "    # batch_count = 0\n",
    "    # for i in tqdm(range(0, len(all_processed_questions), 1), desc=\"Writing to files\"):\n",
    "    #     batch = [all_processed_questions[i]]  # Each batch contains one question in this case\n",
    "        \n",
    "    #     # Alternate between the two output files\n",
    "    #     output_file = output_file_1 if batch_count % 2 == 0 else output_file_2\n",
    "    #     with open(output_file, 'w') as f:\n",
    "    #         json.dump(batch, f, indent=4)\n",
    "    #     batch_count += 1\n",
    "        \n",
    "    #     # Optionally, add a delay to respect the rate limits (1 request per second)\n",
    "    #     time.sleep(1)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'input_data' is your list of questions (from BioASQ or your JSON data)\n",
    "# process_bioasq_data(input_data)\n",
    "for file in ['12B1_golden', '12B2_golden', '12B3_golden', '12B4_golden']:\n",
    "    input_file = f'/kaggle/input/bioasq-12b-golden-enriched/{file}.json'\n",
    "    input_data = json.load(open(input_file))\n",
    "    process_bioasq_data(input_data.get('questions', []), file)\n",
    "# input_file = '/kaggle/input/bioasq-training-12b-json/training12b_new.json'  # Path to your BioASQ dataset\n",
    "# input_data = json.load(open(input_file))\n",
    "# process_bioasq_data(input_data.get('questions', []))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6071368,
     "sourceId": 9886628,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6077039,
     "sourceId": 9894242,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 824.054305,
   "end_time": "2024-11-13T16:51:29.072978",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-13T16:37:45.018673",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
