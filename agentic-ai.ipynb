{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain faiss-cpu transformers datasets gym stable-baselines3 onnx onnxruntime huggingface_hub mlflow tqdm\n",
    "!pip install farm-haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class AgenticAIEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for AgenticAI System using PPO.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, rag_chain, tokenizer, model):\n",
    "        super(AgenticAIEnv, self).__init__()\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.rag_chain = rag_chain\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.current_idx = 0\n",
    "        \n",
    "        # Define the action space (answer or skip)\n",
    "        self.action_space = spaces.Discrete(2)  # 0 = Skip, 1 = Answer\n",
    "        \n",
    "        # Define the observation space (question text)\n",
    "        self.observation_space = spaces.Discrete(len(dataset))\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to a random state.\"\"\"\n",
    "        self.current_idx = np.random.randint(0, len(self.dataset))\n",
    "        return self.dataset[self.current_idx]['question']\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment based on the agent's action.\"\"\"\n",
    "        question = self.dataset[self.current_idx]['question']\n",
    "        ideal_answer = self.dataset[self.current_idx]['ideal_answer']\n",
    "        \n",
    "        if action == 1:  # Action 1: Answer\n",
    "            result = self.rag_chain.run(question)  # Get the model's response\n",
    "            \n",
    "            # Compute the reward based on semantic similarity\n",
    "            reward = compute_semantic_similarity(result, ideal_answer, self.tokenizer, self.model)\n",
    "        else:\n",
    "            reward = 0  # If the agent skips, no reward\n",
    "        \n",
    "        done = self.current_idx == len(self.dataset) - 1  # Episode ends when we've gone through all questions\n",
    "        info = {}  # Additional information (optional)\n",
    "        \n",
    "        # Update the current question index\n",
    "        self.current_idx += 1\n",
    "        if self.current_idx >= len(self.dataset):\n",
    "            self.current_idx = 0  # Loop back to the start\n",
    "        \n",
    "        return question, reward, done, info\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment (optional, e.g., for visualization).\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "def train_agenticai_system(env, total_timesteps=10000):\n",
    "    \"\"\"Train the AgenticAI system using PPO and log everything with MLFlow.\"\"\"\n",
    "    mlflow.start_run()  # Start MLFlow logging\n",
    "    \n",
    "    # Log training parameters\n",
    "    mlflow.log_param('agent_type', 'PPO')\n",
    "    mlflow.log_param('total_timesteps', total_timesteps)\n",
    "    \n",
    "    # Wrap the environment to make it compatible with PPO\n",
    "    env = DummyVecEnv([lambda: env])  # PPO requires the environment to be vectorized\n",
    "    \n",
    "    # Create PPO agent\n",
    "    model = PPO('MlpPolicy', env, verbose=1)\n",
    "    \n",
    "    # Log model parameters and architecture\n",
    "    mlflow.log_param('model', 'MlpPolicy')\n",
    "    \n",
    "    # Train the model and log progress\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    \n",
    "    mlflow.log_metric('training_steps', total_timesteps)  # Log total training steps\n",
    "    mlflow.end_run()  # End MLFlow run\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the custom AgenticAI environment\n",
    "agenticai_env = AgenticAIEnv(train_mediqa, rag_chain, tokenizer, fine_tuned_model)\n",
    "\n",
    "# Train the agent with PPO\n",
    "trained_agent = train_agenticai_system(agenticai_env)\n",
    "\n",
    "# Log agent parameters to MLFlow\n",
    "mlflow.log_param('trained_agent', 'PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "def quantize_onnx_model(model, dummy_input):\n",
    "    \"\"\"Optimize the model using ONNX and log the model size.\"\"\"\n",
    "    \n",
    "    # Convert the PyTorch model to ONNX format\n",
    "    onnx_model_path = './optimized_model.onnx'\n",
    "    torch.onnx.export(model, dummy_input, onnx_model_path)\n",
    "    \n",
    "    # Log the original model size\n",
    "    original_size = os.path.getsize('model.pth') / (1024 * 1024)  # Convert to MB\n",
    "    mlflow.log_metric('original_model_size_MB', original_size)\n",
    "    \n",
    "    # Load the ONNX model\n",
    "    onnx_model = onnx.load(onnx_model_path)\n",
    "    \n",
    "    # Log the ONNX model size\n",
    "    optimized_size = os.path.getsize(onnx_model_path) / (1024 * 1024)  # Convert to MB\n",
    "    mlflow.log_metric('optimized_model_size_MB', optimized_size)\n",
    "    \n",
    "    # Perform quantization (optional, but can improve inference speed)\n",
    "    # Placeholder for quantization code\n",
    "    \n",
    "    return onnx_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_semantic_similarity(result, expected_answer, tokenizer, model):\n",
    "    \"\"\"Compute semantic similarity between result and expected answer.\"\"\"\n",
    "    # Tokenize and get embeddings of result and expected answer\n",
    "    inputs = tokenizer([result, expected_answer], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    embeddings = model.get_input_embeddings()(inputs['input_ids'])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(embeddings[0].detach().numpy().reshape(1, -1), embeddings[1].detach().numpy().reshape(1, -1))\n",
    "    return similarity[0][0]\n",
    "\n",
    "def evaluate_agenticai_system(agent, rag_chain, test_dataset, tokenizer):\n",
    "    \"\"\"Evaluate the AgenticAI system (PPO agent + RAGChain) on the test dataset.\"\"\"\n",
    "    correct_answers = 0\n",
    "    total = len(test_dataset)\n",
    "    \n",
    "    mlflow.start_run()  # Start a new MLFlow run for evaluation\n",
    "    \n",
    "    # Log evaluation parameters\n",
    "    mlflow.log_param('evaluation_dataset', 'MEDIQA')  # Assuming we're using the MEDIQA dataset for evaluation\n",
    "    mlflow.log_param('evaluation_metric', 'semantic_similarity')\n",
    "    \n",
    "    for item in tqdm(test_dataset, desc=\"Evaluating AgenticAI System\"):\n",
    "        question = item['question']\n",
    "        expected_answer = item['ideal_answer']\n",
    "        \n",
    "        # Simulate the agent interacting with the environment (answering the question)\n",
    "        action = agent.predict(question)[0]  # Get the agent's action (0 = Skip, 1 = Answer)\n",
    "        \n",
    "        if action == 1:  # If the action is to answer\n",
    "            result = rag_chain.run(question)  # Get the answer from RAGChain\n",
    "            \n",
    "            # Compute the reward based on semantic similarity\n",
    "            reward = compute_semantic_similarity(result, expected_answer, tokenizer, rag_chain.llm)\n",
    "            \n",
    "            # Log reward for each question to MLFlow\n",
    "            mlflow.log_metric(f'reward_{question[:10]}', reward)  # Log reward for this particular question (truncated to avoid long logs)\n",
    "            \n",
    "            if reward > 0.7:  # Threshold for correct answer based on similarity\n",
    "                correct_answers += 1\n",
    "    \n",
    "    accuracy = correct_answers / total\n",
    "    mlflow.log_metric('AgenticAI_accuracy', accuracy)  # Log accuracy to MLFlow\n",
    "    print(f\"AgenticAI Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    mlflow.end_run()  # End the MLFlow run\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "evaluate_agenticai_system(trained_agent, rag_chain, test_mediqa, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
