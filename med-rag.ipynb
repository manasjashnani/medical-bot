{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9900539,"sourceType":"datasetVersion","datasetId":6081719}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain faiss-gpu transformers evaluate ragas datasets huggingface_hub mlflow tqdm\n# !gym stable-baselines3 onnx onnxruntime\n!pip install sentence-transformers\n!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T03:34:57.692687Z","iopub.execute_input":"2024-11-14T03:34:57.693214Z","iopub.status.idle":"2024-11-14T03:35:14.378606Z","shell.execute_reply.started":"2024-11-14T03:34:57.693158Z","shell.execute_reply":"2024-11-14T03:35:14.377151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install packaging==23.0 mlflow==2.17.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T03:35:14.384263Z","iopub.execute_input":"2024-11-14T03:35:14.384639Z","iopub.status.idle":"2024-11-14T03:35:29.735103Z","shell.execute_reply.started":"2024-11-14T03:35:14.384591Z","shell.execute_reply":"2024-11-14T03:35:29.733339Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EXPORTS AND SETUP","metadata":{}},{"cell_type":"code","source":"import json\nimport mlflow\nfrom datasets import Dataset\nimport torch.cuda\nimport torch\nfrom tqdm import tqdm\nimport mlflow.pytorch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\nimport os\n# from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n# from mlflow.llm.evaluate import evaluate_llm, evaluate_rag\nfrom langchain.chains import RetrievalQA\n# from langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n# from langchain.llms import HuggingFaceLLM\nfrom transformers import default_data_collator\nfrom langchain_core.load import dumpd, dumps, load, loads\nimport pickle\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom __future__ import annotations\nfrom typing import Dict, Optional, Sequence\nfrom langchain.schema import Document\n# from langchain.pydantic_v1 import Extra, root_validator\n\nfrom langchain.callbacks.manager import Callbacks\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n\nfrom sentence_transformers import CrossEncoder\n\nfrom langchain.document_transformers.embeddings_redundant_filter import EmbeddingsRedundantFilter\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipeline\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.document_transformers.long_context_reorder import LongContextReorder\n# from langchain.retrievers.multi_query import MultiQueryRetriever\n\nif torch.cuda.is_available():\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\nelse:\n    torch.set_default_tensor_type('torch.FloatTensor')\n\n# Set default device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nrag_chain_path = 'rag/rag_chain.pkl'\ndirectories = ['fine_tuned_t5_model', 'fine_tuned_t5_tokenizer', 'rag', 'Vectorstore/chromadb']\nfor directory in directories:\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\nexperiment_id = mlflow.create_experiment('BioASQ RAG')\nmlflow.set_experiment('BioASQ RAG')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CREATE DATASETS","metadata":{}},{"cell_type":"code","source":"\ndef load_bioasq_data(train_file, test_file):\n    \"\"\"Load and process BioASQ data, utilizing all fields (question, ideal_answer, articles, snippets).\"\"\"\n    \n    def build_dataset(data):\n        dataset = []\n        for item in data:\n            question = item['question']\n            ideal_answer = item['ideal_answer']\n            \n            context = \"\"\n            for article in item.get('articles', []):\n                title = article.get('title', '')  # Default to empty string if 'title' is missing\n                abstract = article.get('abstract', '')  # Default to empty string if 'abstract' is missing\n                context += f\"{title} {abstract} \"  # Concatenate safely\n            \n            for snippet in item.get('snippets', []):\n                title = snippet.get('title', '')  # Default to empty string if 'title' is missing\n                abstract = snippet.get('abstract', '')  # Default to empty string if 'abstract' is missing\n                context += f\"{title} {abstract} \"  # Concatenate safely\n            \n            context += \" \".join(item.get('concepts', []))  # Ensure concepts are strings\n            \n            # Construct train example\n            dataset.append({\n                'question': question,\n                'context': context,\n                'ideal_answer': ideal_answer\n            })\n\n\n        return dataset\n\n    with open(train_file, 'r') as f:\n        train_data = json.load(f)\n    \n    with open(test_file, 'r') as f:\n        test_data = json.load(f)\n\n    train_dataset = build_dataset(train_data)\n    test_dataset = build_dataset(test_data)\n\n    return train_dataset, test_dataset\n\n\n# Initialize MLFlow logging for the entire pipeline\nwith mlflow.start_run(run_name=\"Dataset Creation\") as dataset_creation_run:\n\n\n    # Load BioASQ training and testing datasets\n    train_file = '/kaggle/input/bio-asq/training12b_train.json'\n    test_file = '/kaggle/input/bio-asq/training12b_test.json'\n    train_dataset, test_dataset = load_bioasq_data(train_file, test_file)\n\n    # Optionally log dataset info to MLFlow\n    mlflow.log_param('bio_asq_train_dataset_size', len(train_dataset))\n    mlflow.log_param('bio_asq_test_dataset_size', len(test_dataset))\n\n    # Save datasets as JSON files\n    train_output_file = 'bio_asq_train_dataset.json'\n    test_output_file = 'bio_asq_test_dataset.json'\n\n    with open(train_output_file, 'w') as f:\n        json.dump(train_dataset, f, indent=4)\n\n    with open(test_output_file, 'w') as f:\n        json.dump(test_dataset, f, indent=4)\n\n    # Log the file paths to MLFlow for tracking\n    mlflow.log_artifact(train_output_file)\n    mlflow.log_artifact(test_output_file)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T03:35:39.676514Z","iopub.status.idle":"2024-11-14T03:35:39.677013Z","shell.execute_reply.started":"2024-11-14T03:35:39.676775Z","shell.execute_reply":"2024-11-14T03:35:39.676797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"FINE TUNE LLM","metadata":{}},{"cell_type":"code","source":"\ndef fine_tune_llm(train_dataset, test_dataset, model_name='t5-small'):\n    \"\"\"Fine-tune a pre-trained LLM on BioASQ data.\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    model.to(device)\n    tokenizer = T5Tokenizer.from_pretrained(model_name)\n\n    # Tokenize the questions and answers, include context for the input\n    train_encodings = tokenizer([f\"question: {item['question']} context: {item['context']}\" for item in train_dataset],\n                                truncation=True, padding=True, max_length=512)\n    train_labels = tokenizer([item['ideal_answer'] for item in train_dataset], truncation=True, padding=True, max_length=512)\n\n    test_encodings = tokenizer([f\"question: {item['question']} context: {item['context']}\" for item in test_dataset],\n                               truncation=True, padding=True, max_length=512)\n    test_labels = tokenizer([item['ideal_answer'] for item in test_dataset], truncation=True, padding=True, max_length=512)\n    \n    # Convert to Huggingface Dataset format\n    train_data = Dataset.from_dict({\n        'input_ids': train_encodings['input_ids'],\n        'attention_mask': train_encodings['attention_mask'],\n        'labels': train_labels['input_ids']\n    })\n    \n    test_data = Dataset.from_dict({\n        'input_ids': test_encodings['input_ids'],\n        'attention_mask': test_encodings['attention_mask'],\n        'labels': test_labels['input_ids']\n    })\n\n    print(f'Model is on device: {next(model.parameters()).device}')\n\n    # Print a tensor's device\n    # print(f'Example tensor device: {train_data[0][\"input_ids\"].device}')\n\n    def move_to_device(batch):\n        return {\n            'input_ids': torch.tensor(batch['input_ids']).to(device),\n            'attention_mask': torch.tensor(batch['attention_mask']).to(device),\n            'labels': torch.tensor(batch['labels']).to(device)\n        }\n\n    train_data = train_data.map(move_to_device, batched=True)\n    test_data = test_data.map(move_to_device, batched=True)\n\n    # print(f'Train data is on device: {train_data[0][\"input_ids\"].device}')\n    # print(f'Test data is on device: {test_data[0][\"input_ids\"].device}')\n\n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir='./results',\n        evaluation_strategy=\"steps\",\n        learning_rate=5e-5,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n        save_steps=100,\n        save_total_limit=3,\n        load_best_model_at_end=True,\n        report_to=\"mlflow\",\n        seed=42,\n        fp16=True,\n        gradient_accumulation_steps=2,\n        dataloader_pin_memory=True,\n        dataloader_num_workers=4\n    )\n\n    torch.manual_seed(training_args.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(training_args.seed)  \n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_data,\n        eval_dataset=test_data,\n        tokenizer=tokenizer,\n        data_collator=default_data_collator\n    )\n    \n    # Log model training parameters\n    mlflow.log_param('model_name', model_name)\n    mlflow.log_param('epochs', 3)\n    mlflow.log_param('batch_size', 8)\n    \n    trainer.train()\n    \n    mlflow.log_metric('training_loss', trainer.state.best_metric)  # Log training loss\n    \n    return model, tokenizer\n\nwith mlflow.start_run(run_name=\"Fine Tuning\", parent_run_id=dataset_creation_run.info.run_id) as fine_tuning_run:\n    # Fine-tune the model on the BioASQ dataset\n    model_name = 't5-small'  # Or any other suitable pre-trained model\n    fine_tuned_t5_model, fine_tuned_t5_tokenizer = fine_tune_llm(train_dataset, test_dataset, model_name)\n\n    # Log model parameters to MLFlow\n    mlflow.log_param('fine_tuned_t5_model_name', model_name)\n    mlflow.pytorch.log_model(fine_tuned_t5_model, 'fine_tuned_t5_model')\n\n    fine_tuned_t5_model.save_pretrained('fine_tuned_t5_model')\n    fine_tuned_t5_tokenizer.save_pretrained('fine_tuned_t5_tokenizer')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EVALUATE LLM","metadata":{}},{"cell_type":"code","source":"from evaluate import load\n# Load the ROUGE metric\nimport evaluate\n\nrouge = load(\"rouge\")\n\n# Function to evaluate the fine-tuned model using Hugging Face's evaluator\ndef evaluate_llm_with_huggingface(model, tokenizer, test_dataset):\n    predictions = []\n    references = []\n    \n    for item in tqdm(test_dataset, desc=\"Evaluating LLM\"):\n        question = item['question']\n        context = item['context']\n        ideal_answer = item['ideal_answer']\n        \n        # Prepare the input for the model\n        input_text = f\"question: {question} context: {context}\"\n        inputs = tokenizer(input_text, return_tensors='pt')\n        \n        # Generate answer\n        output_ids = model.generate(inputs['input_ids'], max_length=150)\n        generated_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        \n        # Store predictions and references\n        predictions.append(generated_answer)\n        references.append(ideal_answer)\n    \n    # Evaluate using ROUGE\n    result = rouge.compute(predictions=predictions, references=references)\n    mlflow.log_metric('fine_tuned_t5_rouge_scores', result)\n    print(\"ROUGE scores:\", result)\n    \n    \n    return result\n\n\nwith mlflow.start_run(run_name=\"LLM Evaluation\", parent_run_id=fine_tuning_run.info.run_id) as llm_evaluation_run:\n    llm_metrics = evaluate_llm_with_huggingface(fine_tuned_t5_model, fine_tuned_t5_tokenizer, test_dataset)\n    print(f\"LLM Evaluation Metrics: {llm_metrics}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CREATE RAG DATASET","metadata":{}},{"cell_type":"code","source":"\ndef create_rag_dataset(train_data, test_data):\n    \"\"\"Create a dataset for RAG from BioASQ data.\"\"\"\n    \n    def build_dataset(data):\n        dataset = []\n\n        # Create text splitter for RAG\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 1250,\n            chunk_overlap = 100,\n            length_function = len,\n            is_separator_regex = False\n        )\n        \n        for item in data:\n            chunks = text_splitter.split_text(item['context'])\n            \n            # Add each chunk as a separate document with metadata\n            for chunk in chunks:\n                dataset.append({\n                    'context': chunk,\n                    'metadata': {\n                        'question': item['question'],\n                        'ideal_answer': item['ideal_answer']\n                    }\n                })\n\n        return dataset\n\n    train_dataset = build_dataset(train_data)\n    test_dataset = build_dataset(test_data)\n\n    return train_dataset, test_dataset\n\n\n# Initialize MLFlow logging for the entire pipeline\nwith mlflow.start_run(run_name=\"Dataset Creation for RAG\", parent_run_id=llm_evaluation_run.info.run_id) as rag_dataset_creation_run:\n\n    train_dataset_rag, test_dataset_rag = load_bioasq_data(train_dataset, test_dataset)\n\n    # Save datasets as JSON files\n    train_output_file = 'bio_asq_train_dataset_rag.json'\n    test_output_file = 'bio_asq_test_dataset_rag.json'\n\n    train_string = dumps(train_dataset_rag, pretty=True)\n    test_string = dumps(test_dataset_rag, pretty=True)\n\n    with open(train_output_file, 'w') as f:\n        json.dump(train_string, f, indent=4)\n\n    with open(test_output_file, 'w') as f:\n        json.dump(test_string, f, indent=4)\n\n    # Log the file paths to MLFlow for tracking\n    mlflow.log_artifact(train_output_file)\n    mlflow.log_artifact(test_output_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CREATE RAG PIPELINE","metadata":{}},{"cell_type":"code","source":"\nclass BgeRerank(BaseDocumentCompressor):\n    model_name:str = 'BAAI/bge-reranker-large'\n    \"\"\"Model name to use for reranking.\"\"\"\n    top_n: int = 3\n    \"\"\"Number of documents to return.\"\"\"\n    model:CrossEncoder = CrossEncoder(model_name)\n    \"\"\"CrossEncoder instance to use for reranking.\"\"\"\n\n    def bge_rerank(self, query,docs):\n        model_inputs =  [[query, doc] for doc in docs]\n        scores = self.model.predict(model_inputs)\n        results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n        return results[:self.top_n]\n\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n        arbitrary_types_allowed = True\n\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks: Optional[Callbacks] = None,\n    ) -> Sequence[Document]:\n        \"\"\"\n        Compress documents using BAAI/bge-reranker models.\n\n        Args:\n            documents: A sequence of documents to compress.\n            query: The query to use for compressing the documents.\n            callbacks: Callbacks to run during the compression process.\n\n        Returns:\n            A sequence of compressed documents.\n        \"\"\"\n        if len(documents) == 0:  # to avoid empty api call\n            return []\n        doc_list = list(documents)\n        _docs = [d.page_content for d in doc_list]\n        results = self.bge_rerank(query, _docs)\n        final_results = []\n        for r in results:\n            doc = doc_list[r[0]]\n            doc.metadata[\"relevance_score\"] = r[1]\n            final_results.append(doc)\n        return final_results\n    \ndef create_rag_pipeline(train_dataset_rag):\n    \"\"\"Create a RAGChain pipeline using the fine-tuned model and ChromaDB.\"\"\"\n\n    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n    # Create Chroma index\n    chroma_dir = 'Vectorstore/chromadb'\n    vectorstore = Chroma(embedding_function = embedding_model,\n                         persist_directory = chroma_dir,\n                     collection_name = \"bioasq_train_documents\")\n\n    vectorstore.add_documents(train_dataset_rag)\n    vectorstore.persist()\n\n    mlflow.log_param('rag_vectorstore', vectorstore)\n    mlflow.log_artifact(chroma_dir)\n\n    bm25_retriever = BM25Retriever.from_documents(train_dataset_rag)\n    bm25_retriever.k=10    \n\n    vs_retriever = vectorstore.as_retriever(search_kwargs = {\"k\":10})\n\n    ensemble_retriever = EnsembleRetriever(retrievers = [bm25_retriever,vs_retriever], weight = [0.5,0.5])\n\n    redundant_filter = EmbeddingsRedundantFilter(embeddings=embedding_model)\n\n    reordering = LongContextReorder()\n\n    reranker = BgeRerank()\n\n    pipeline_compressor = DocumentCompressorPipeline(transformers = [redundant_filter, reordering, reranker])\n\n    compression_pipeline = ContextualCompressionRetriever(base_compressor = pipeline_compressor, base_retriever = ensemble_retriever)\n\n    qa_advanced = RetrievalQA.from_chain_type(llm = fine_tuned_t5_model,\n                                    chain_type = \"stuff\",\n                                    retriever = compression_pipeline,\n                                    return_source_documents = True)\n    mlflow.log_param('rag_chain_created', True)\n    return qa_advanced\n\ndef save_rag_pipeline(rag_chain, rag_chain_path):\n    \"\"\"Save the RAGChain pipeline to a file.\"\"\"\n    with open(rag_chain_path, 'wb') as f:\n        pickle.dump(rag_chain, f)\n\n    mlflow.log_artifact(rag_chain_path)\n\nwith mlflow.start_run(run_name=\"RAG Pipeline Creation\", parent_run_id=rag_dataset_creation_run.info.run_id) as rag_pipeline_creation_run:\n    rag_chain = create_rag_pipeline(train_dataset_rag)\n    save_rag_pipeline(rag_chain, rag_chain_path)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EVALUATE RAG PIPELINE","metadata":{}},{"cell_type":"code","source":"\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    answer_correctness,\n    context_recall,\n    context_precision,\n)\n\ndef evaluate_rag_pipeline(rag_chain, test_dataset_rag):\n    \"\"\"Evaluate the RAGChain pipeline using the test dataset.\"\"\"\n    questions = []\n    answers = []\n    contexts = []\n    ground_truths = []\n\n    for item in tqdm(test_dataset_rag, desc=\"Evaluating RAG Pipeline\"):\n        question = item['question']\n        ideal_answer = item['ideal_answer']\n        result = rag_chain.invoke({\"query\": question})\n        questions.append(question)\n        answers.append(result['result'])\n        contexts.append([context.page_content for context in result['source_documents']])\n        ground_truths.append(ideal_answer)\n\n    response_dataset = Dataset.from_dict({\n        \"question\" : questions,\n        \"answer\" : answers,\n        \"contexts\" : contexts,\n        \"ground_truth\" : ground_truths\n    })\n\n    metrics = [\n        faithfulness,\n        answer_relevancy,\n        context_recall,\n        context_precision,\n        answer_correctness,\n    ]\n    #\n    eval_results = evaluate(response_dataset, metrics,raise_exceptions=False)\n    \n    mlflow.log_metric('rag_pipeline_evaluation', eval_results)\n    return eval_results\n\nwith mlflow.start_run(run_name=\"RAG Pipeline Evaluation\", parent_run_id=rag_pipeline_creation_run.info.run_id) as rag_pipeline_evaluation_run:\n    eval_results = evaluate_rag_pipeline(rag_chain, test_dataset_rag)\n    print(f\"RAG Pipeline Evaluation Metrics: {eval_results}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}