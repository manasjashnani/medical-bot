{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9900539,"sourceType":"datasetVersion","datasetId":6081719},{"sourceId":9994606,"sourceType":"datasetVersion","datasetId":6151494},{"sourceId":9995756,"sourceType":"datasetVersion","datasetId":6151500},{"sourceId":9997134,"sourceType":"datasetVersion","datasetId":6153063},{"sourceId":10019908,"sourceType":"datasetVersion","datasetId":6169793},{"sourceId":169957,"sourceType":"modelInstanceVersion","modelInstanceId":144589,"modelId":167151},{"sourceId":169960,"sourceType":"modelInstanceVersion","modelInstanceId":144592,"modelId":167154},{"sourceId":174515,"sourceType":"modelInstanceVersion","modelInstanceId":148583,"modelId":171110},{"sourceId":174516,"sourceType":"modelInstanceVersion","modelInstanceId":148584,"modelId":171111}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!lscpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T03:58:00.879738Z","iopub.execute_input":"2024-11-28T03:58:00.880091Z","iopub.status.idle":"2024-11-28T03:58:01.997949Z","shell.execute_reply.started":"2024-11-28T03:58:00.880063Z","shell.execute_reply":"2024-11-28T03:58:01.996755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langchain faiss-gpu transformers evaluate datasets huggingface_hub mlflow tqdm chromadb\n!pip install rank_bm25\n!pip install onnxruntime-gpu\n# !gym stable-baselines3 onnx onnxruntime\n# !pip install onnx onnxruntime\n!pip install optimum[onnxruntime] onnx\n!pip install sentence-transformers\n!pip install rouge_score\n!pip install -U langsmith openai\n!pip install langchain-openai ragas\n!pip install -U langsmith openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T03:58:02.000264Z","iopub.execute_input":"2024-11-28T03:58:02.000648Z","iopub.status.idle":"2024-11-28T04:00:13.237090Z","shell.execute_reply.started":"2024-11-28T03:58:02.000602Z","shell.execute_reply":"2024-11-28T04:00:13.235979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install packaging==23.0 mlflow==2.17.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T04:00:13.238524Z","iopub.execute_input":"2024-11-28T04:00:13.238891Z","iopub.status.idle":"2024-11-28T04:00:26.265055Z","shell.execute_reply.started":"2024-11-28T04:00:13.238858Z","shell.execute_reply":"2024-11-28T04:00:26.264178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EXPORTS AND SETUP","metadata":{}},{"cell_type":"code","source":"import json\nimport mlflow\nfrom datasets import Dataset as HFDataset\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n# from optimum.intel import IncQuantizer\n# from torch.ao.quantization import get_default_qconfig, quantize_dynamic\nimport torch.cuda\nimport random\nfrom tqdm import tqdm\nimport mlflow.pytorch\n# import onnx\n# from onnxruntime.quantization import quantize_static, QuantizationMode, CalibrationDataReader, QuantType, QuantFormat\n# import onnxruntime as ort\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\nfrom optimum.onnxruntime.configuration import AutoQuantizationConfig\nfrom transformers import pipeline, ElectraForQuestionAnswering, ElectraTokenizer, AlbertForQuestionAnswering, AlbertTokenizer, AutoModelForQuestionAnswering, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\nfrom transformers import AutoTokenizer, AutoModel\n\nimport os\nimport time\n# from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nimport pandas as pd\n# from mlflow.llm.evaluate import evaluate_llm, evaluate_rag\nfrom langchain.chains import (\n    RetrievalQA, StuffDocumentsChain, LLMChain, create_retrieval_chain\n)\n# from langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.embeddings.base import Embeddings\n# from langchain.llms import HuggingFaceLLM\nfrom langchain.llms.base import LLM\nfrom transformers import default_data_collator\nfrom langchain_core.runnables.base import coerce_to_runnable\nfrom langchain_core.load import dumpd, dumps, load, loads\nimport pickle\nimport uuid\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.retrievers import (\n    BM25Retriever, EnsembleRetriever, ContextualCompressionRetriever\n)\nfrom __future__ import annotations\nfrom typing import Dict, Optional, Sequence\nfrom langchain.schema import Document\nfrom accelerate import Accelerator\nfrom torch.cuda.amp import autocast\n# from langchain.pydantic_v1 import Extra, root_validator\nfrom pydantic import Extra\n\nfrom langchain.callbacks.manager import Callbacks\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate\nfrom sentence_transformers import CrossEncoder\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\nfrom langchain import hub\nfrom langchain.document_transformers.embeddings_redundant_filter import EmbeddingsRedundantFilter\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipeline\nfrom langchain.document_transformers.long_context_reorder import LongContextReorder\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai import OpenAIEmbeddings\n\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ[\"OPENAI_API_KEY\"] = user_secrets.get_secret(\"OpenAI API Key\")\n\nos.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\nos.environ[\"LANGCHAIN_API_KEY\"]=user_secrets.get_secret(\"LangChain-API-key\")\nos.environ[\"LANGCHAIN_PROJECT\"]=\"Bio-ASQ-RAG-LangSmith-eval\"\n\nquantize_llm = True\neval_llm = False\nrag_chain_path = 'rag/rag_chain.pkl'\nquantized_model_path = 'quantized_model'\nquantized_tokenizer_path = 'quantized_tokenizer'\nchroma_dir = 'Vectorstore/chromadb3'\nchroma_dir_in = 'Vectorstore/chromadb3'\n\n# /kaggle/input/bioasq-rag-chromadb-vector-store/chromadb\n    \ndirectories = ['fine_tuned_model', quantized_model_path, quantized_tokenizer_path, 'fine_tuned_tokenizer', 'rag', chroma_dir, 'fine-tune-llm-results', 'fine-tune-llm-logs']\nfor directory in directories:\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\nexperiment_id = mlflow.create_experiment('BioASQ RAG')\nmlflow.set_experiment('BioASQ RAG')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T04:00:26.267239Z","iopub.execute_input":"2024-11-28T04:00:26.267578Z","iopub.status.idle":"2024-11-28T04:00:51.446137Z","shell.execute_reply.started":"2024-11-28T04:00:26.267547Z","shell.execute_reply":"2024-11-28T04:00:51.445338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CREATE DATASETS","metadata":{}},{"cell_type":"code","source":"fine_tune_llm_train_input_file = '/kaggle/input/bioasq-fine-tune-llm-data/bio_asq_train_dataset.json'\nfine_tune_llm_test_input_file = '/kaggle/input/bioasq-fine-tune-llm-data/bio_asq_test_dataset.json'\n\ndef load_fine_tune_llm_datasets():\n    if os.path.exists(fine_tune_llm_train_input_file) and os.path.exists(fine_tune_llm_test_input_file):\n        with open(fine_tune_llm_train_input_file, 'r') as f:\n            train_data = json.load(f)\n        with open(fine_tune_llm_test_input_file, 'r') as f:\n            test_data = json.load(f)\n        return train_data, test_data\n    return None, None\n\ndef load_bioasq_data(train_file, test_file):\n    \"\"\"Load and process BioASQ data, utilizing all fields (question, ideal_answer, articles, snippets).\"\"\"\n    \n    def build_dataset(data):\n        dataset = []\n        for item in data:\n            question = item['question']\n            ideal_answer = item['ideal_answer']\n            \n            context = \"\"\n            for article in item.get('articles', []):\n                title = article.get('title', '')  # Default to empty string if 'title' is missing\n                abstract = article.get('abstract', '')  # Default to empty string if 'abstract' is missing\n                context += f\"{title} {abstract} \"  # Concatenate safely\n            \n            for snippet in item.get('snippets', []):\n                title = snippet.get('title', '')  # Default to empty string if 'title' is missing\n                abstract = snippet.get('abstract', '')  # Default to empty string if 'abstract' is missing\n                context += f\"{title} {abstract} \"  # Concatenate safely\n            \n            context += \" \".join(item.get('concepts', []))  # Ensure concepts are strings\n            \n            # Construct train example\n            dataset.append({\n                'question': question,\n                'context': context,\n                'ideal_answer': ideal_answer\n            })\n\n\n        return dataset\n\n    with open(train_file, 'r') as f:\n        train_data = json.load(f)\n    \n    with open(test_file, 'r') as f:\n        test_data = json.load(f)\n\n    train_dataset = build_dataset(train_data)\n    test_dataset = build_dataset(test_data)\n\n    return train_dataset, test_dataset\n\n\n# Initialize MLFlow logging for the entire pipeline\nmlflow.autolog()\nwith mlflow.start_run(run_name=\"Dataset Creation\") as dataset_creation_run:\n    train_dataset, test_dataset = load_fine_tune_llm_datasets()\n    if train_dataset:\n        print('Fine tune LLM dataset loaded')\n        mlflow.log_param('Fine tune LLM dataset loaded', True)\n    else:\n        print('Fine tune LLM dataset Creation')\n        mlflow.log_param('Fine tune LLM dataset loaded', False)\n        # Load BioASQ training and testing datasets\n        train_file = '/kaggle/input/bio-asq/training12b_train.json'\n        test_file = '/kaggle/input/bio-asq/training12b_test.json'\n        train_dataset, test_dataset = load_bioasq_data(train_file, test_file)\n    \n        # Optionally log dataset info to MLFlow\n        mlflow.log_param('bio_asq_train_dataset_size', len(train_dataset))\n        mlflow.log_param('bio_asq_test_dataset_size', len(test_dataset))\n    \n        # Save datasets as JSON files\n        train_output_file = 'bio_asq_train_dataset.json'\n        test_output_file = 'bio_asq_test_dataset.json'\n    \n        with open(train_output_file, 'w') as f:\n            json.dump(train_dataset, f, indent=4)\n    \n        with open(test_output_file, 'w') as f:\n            json.dump(test_dataset, f, indent=4)\n    \n        # Log the file paths to MLFlow for tracking\n        mlflow.log_artifact(train_output_file)\n        mlflow.log_artifact(test_output_file)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T04:00:51.447604Z","iopub.execute_input":"2024-11-28T04:00:51.448250Z","iopub.status.idle":"2024-11-28T04:00:54.194599Z","shell.execute_reply.started":"2024-11-28T04:00:51.448222Z","shell.execute_reply":"2024-11-28T04:00:54.193739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"FINE TUNE LLM","metadata":{}},{"cell_type":"code","source":"finetuned_model_path = '/kaggle/input/bioasq-fine-tuned-t5-small/transformers/default/1'\nfinetuned_tokenizer_path = '/kaggle/input/bioasq-fine-tuned-t5-small-tokenizer/transformers/default/1'\n\nquantized_finetuned_model_path = '/kaggle/input/bioasq-quantized-fine-tuned-t5-small/onnx/default/1'\nquantized_finetuned_tokenizer_path = '/kaggle/input/bioasq-quantized-fine-tuned-t5-small-tokenizer/transformers/default/1'\n\nclass BioASQDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n        return item\n    \n\ndef fine_tune_llm(train_dataset, test_dataset, model_name='t5-small'):\n    \"\"\"Fine-tune a pre-trained LLM on BioASQ data.\"\"\"\n    \n    if os.path.exists(finetuned_model_path) and os.path.exists(finetuned_tokenizer_path):\n        print('Loading fine-tuned model and tokenizer from checkpoint files')\n        model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path)\n        tokenizer = T5Tokenizer.from_pretrained(finetuned_tokenizer_path)\n        return model, tokenizer\n    else:\n        # Load model and tokenizer from Hugging Face\n        model = T5ForConditionalGeneration.from_pretrained(model_name)\n        tokenizer = T5Tokenizer.from_pretrained(model_name)\n\n    print(f'Loaded model {model_name} and tokenizer from Hugging Face')\n\n    # Tokenize the questions and answers, include context for the input\n    train_encodings = tokenizer([f\"question: {item['question']} context: {item['context']}\" for item in train_dataset],\n                                truncation=True, padding=True, max_length=256)\n    train_labels = tokenizer([item['ideal_answer'] for item in train_dataset], truncation=True, padding=True, max_length=256)\n\n    test_encodings = tokenizer([f\"question: {item['question']} context: {item['context']}\" for item in test_dataset],\n                               truncation=True, padding=True, max_length=256)\n    test_labels = tokenizer([item['ideal_answer'] for item in test_dataset], truncation=True, padding=True, max_length=256)\n\n    train_data = BioASQDataset(train_encodings, train_labels)\n    test_data = BioASQDataset(test_encodings, test_labels)\n\n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir='/fine-tune-llm-results',\n        evaluation_strategy=\"steps\",\n        learning_rate=5e-5,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        logging_dir='/fine-tune-llm-logs',\n        logging_steps=10,\n        save_steps=100,\n        save_total_limit=3,\n        load_best_model_at_end=True,\n        seed=42,\n        data_seed=42,\n        report_to=\"none\",\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_data,\n        eval_dataset=test_data,\n        tokenizer=tokenizer,\n    )\n\n    # Train the model\n    trainer.train()\n\n    return model, tokenizer\n\n\n# Fine-tune the model on BioASQ dataset\nmlflow.autolog()\nwith mlflow.start_run(run_name=\"Fine Tuning\") as fine_tuning_run:\n    if os.path.exists(quantized_finetuned_model_path) and os.path.exists(quantized_finetuned_tokenizer_path) and quantize_llm:\n        print('Loading quantized_fine-tuned model and tokenizer from checkpoint files')\n        llm = ORTModelForSeq2SeqLM.from_pretrained(quantized_finetuned_model_path)\n        tokenizer = T5Tokenizer.from_pretrained(quantized_finetuned_tokenizer_path)\n    else:\n        model_name = 't5-small'\n        fine_tuned_model, fine_tuned_tokenizer = fine_tune_llm(train_dataset, test_dataset, model_name)\n\n        # Save the fine-tuned model\n        fine_tuned_model.save_pretrained('fine_tuned_model')\n        fine_tuned_tokenizer.save_pretrained('fine_tuned_tokenizer')\n\n        # Log model parameters to MLFlow\n        mlflow.log_param('fine_tuned_model_name', model_name)\n        mlflow.log_param('quantize_llm', quantize_llm)\n\n        # Save model weights to MLFlow\n        torch.save(fine_tuned_model.state_dict(), 'fine_tuned_model_weights.pth')\n        mlflow.log_artifact('fine_tuned_model_weights.pth', artifact_path='fine_tuned_model')\n        \n        if quantize_llm:\n            print('Quantizing llm model')\n            llm = ORTModelForSeq2SeqLM.from_pretrained(\n                finetuned_model_path,\n                from_transformers=True,\n                export=True,  # Exports the model to ONNX\n                # quantization_config=AutoQuantizationConfig.default()\n            )\n            tokenizer = T5Tokenizer.from_pretrained(finetuned_tokenizer_path)\n            \n            # Save the quantized model\n            llm.save_pretrained(quantized_model_path)\n            tokenizer.save_pretrained(quantized_tokenizer_path)\n        else:\n            llm = fine_tuned_model\n            tokenizer = fine_tuned_tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T04:00:54.195897Z","iopub.execute_input":"2024-11-28T04:00:54.196171Z","iopub.status.idle":"2024-11-28T04:01:00.851447Z","shell.execute_reply.started":"2024-11-28T04:00:54.196145Z","shell.execute_reply":"2024-11-28T04:01:00.850756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EVALUATE LLM","metadata":{}},{"cell_type":"code","source":"from evaluate import load\n# Load the ROUGE metric\nimport evaluate\n\nrouge = load(\"rouge\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef chunk_text(text, max_tokens, tokenizer):\n    \"\"\"Split text into chunks within the token limit.\"\"\"\n    tokens = tokenizer.tokenize(text)\n    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n    return [tokenizer.convert_tokens_to_string(chunk) for chunk in chunks]\n\ndef generate_from_chunks(question, context, tokenizer, model, max_tokens=512):\n    \"\"\"Generate an answer by aggregating outputs from context chunks.\"\"\"\n    chunks = chunk_text(context, max_tokens - len(tokenizer.tokenize(question)) - 10, tokenizer)\n    answers = []\n    for chunk in chunks:\n        input_text = f\"question: {question} context: {chunk}\"\n        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_tokens, truncation=True)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        # outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n        outputs = model.generate(\n            inputs['input_ids'],\n            max_length=150, \n            num_beams=5,  # Beam width\n            early_stopping=True, \n            length_penalty=1.0  # Control length balance\n        )\n        answers.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n    return \" \".join(answers)  # Combine chunk outputs\n\n# Function to evaluate the fine-tuned model using Hugging Face's evaluator\ndef evaluate_llm_with_huggingface(model, tokenizer, test_dataset):\n    predictions = []\n    references = []\n    question_ans = 1\n    for item in tqdm(test_dataset, desc=\"Evaluating LLM\"):\n        print(f'Answering question {question_ans}')\n        question_ans += 1\n        question = item['question']\n        context = item['context']\n        ideal_answer = item['ideal_answer']\n        \n        torch.cuda.empty_cache()\n        \n        # Move the quantized model to the same device\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model.to(device)\n\n        \n        # Generate answer\n        with autocast():\n            pred = generate_from_chunks(question, context, tokenizer, model)\n        \n        # Store predictions and references\n        predictions.append(pred)\n        references.append(ideal_answer)\n    \n    # Evaluate using ROUGE\n    result = rouge.compute(predictions=predictions, references=references)\n    for key, value in result.items():\n        mlflow.log_metric(key, value)\n    print(\"ROUGE scores:\", result)\n    \n    return result\n\nmlflow.autolog()\nwith mlflow.start_run(run_name=\"LLM Evaluation\", parent_run_id=fine_tuning_run.info.run_id) as llm_evaluation_run:\n    mlflow.log_param('eval_llm', eval_llm)\n    if eval_llm:\n        llm_metrics = evaluate_llm_with_huggingface(llm, tokenizer, test_dataset)\n        print(f\"LLM Evaluation Metrics: {llm_metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T04:01:00.852877Z","iopub.execute_input":"2024-11-28T04:01:00.853362Z","iopub.status.idle":"2024-11-28T04:01:01.956090Z","shell.execute_reply.started":"2024-11-28T04:01:00.853324Z","shell.execute_reply":"2024-11-28T04:01:01.955470Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CREATE RAG DATASET","metadata":{}},{"cell_type":"code","source":"# rag_train_input_file = '/kaggle/input/bioasq-rag-data/bio_asq_train_dataset_rag.json'\n# rag_test_input_file = '/kaggle/input/bioasq-rag-data/bio_asq_test_dataset_rag.json'\n\n# def load_rag_datasets():\n#     if os.path.exists(rag_train_input_file) and os.path.exists(rag_test_input_file):\n#         with open(rag_train_input_file, 'r') as f:\n#             train_data = json.load(f)\n#         with open(rag_test_input_file, 'r') as f:\n#             test_data = json.load(f)\n#         return train_data, test_data\n#     return None, None\n\nrag_train_input_file = '/kaggle/input/bioasq-rag-data/bio_asq_train_dataset_rag.pkl'\nrag_test_input_file = '/kaggle/input/bioasq-rag-data/bio_asq_test_dataset_rag.pkl'\n\ndef load_rag_datasets():\n    if os.path.exists(rag_train_input_file) and os.path.exists(rag_test_input_file):\n        with open(rag_train_input_file, 'rb') as f:\n            train_data = pickle.load(f)\n        with open(rag_test_input_file, 'rb') as f:\n            test_data = pickle.load(f)\n        return train_data, test_data\n    return None, None\n\n\n\ndef create_rag_dataset(train_data, test_data):\n    \"\"\"Create a dataset for RAG from BioASQ data.\"\"\"\n    \n    def build_dataset(data):\n        dataset = []      \n\n        # Create text splitter for RAG\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 450,\n            chunk_overlap = 50,\n            length_function = len,\n            is_separator_regex = False\n        )\n\n        # docs = [\n        #     Document(\n        #         page_content = item['context'],\n        #         metadata = {\n        #             'question': item['question'],\n        #             'ideal_answer': item['ideal_answer']\n        #         }\n        #     ) for item in data\n        # ]\n        \n        # split_docs = text_splitter.split_text(docs)\n\n        # for i, chunk in enumerate(split_docs):\n        #     chunk.id = str(uuid.uuid4())\n        #     chunk.metadata[\"length\"] = len(chunk.page_content)\n        #     dataset.append(chunk)\n\n        # id = 0\n        \n        for item in data:\n            chunks = text_splitter.split_text(item['context'])\n            \n            # Add each chunk as a separate document with metadata\n            for chunk in chunks:\n                dataset.append(\n                    Document(\n                        id = str(uuid.uuid4()),\n                        page_content = chunk,\n                        metadata = {\n                            'question': item['question'],\n                            'ideal_answer': item['ideal_answer'],\n                            'length': len(chunk)\n                        }\n                    )\n                )\n\n        return dataset\n\n    train_dataset = build_dataset(train_data)\n    test_dataset = build_dataset(test_data)\n\n    return train_dataset, test_dataset\n\n\n# Initialize MLFlow logging for the entire pipeline\nmlflow.autolog()\nwith mlflow.start_run(run_name=\"Dataset Creation for RAG\", parent_run_id=llm_evaluation_run.info.run_id) as rag_dataset_creation_run:\n    train_dataset_rag, test_dataset_rag = load_rag_datasets()\n    if train_dataset_rag:\n        print('RAG dataset loaded')\n        mlflow.log_param('RAG dataset loaded', True)\n        \n    else:\n        print('RAG dataset Creation')\n        train_dataset_rag, test_dataset_rag = create_rag_dataset(train_dataset, test_dataset)\n\n        train_output_file = 'bio_asq_train_dataset_rag.pkl'\n        test_output_file = 'bio_asq_test_dataset_rag.pkl'\n        \n        # # Save datasets as JSON files\n        # train_output_file = 'bio_asq_train_dataset_rag.json'\n        # test_output_file = 'bio_asq_test_dataset_rag.json'\n    \n        # train_string = dumps(train_dataset_rag, pretty=True)\n        # test_string = dumps(test_dataset_rag, pretty=True)\n    \n        # with open(train_output_file, 'w') as f:\n        #     json.dump(train_string, f, indent=4)\n    \n        # with open(test_output_file, 'w') as f:\n        #     json.dump(test_string, f, indent=4)\n\n        with open(train_output_file, 'wb') as f:\n            pickle.dump(train_dataset_rag, f)\n    \n        with open(test_output_file, 'wb') as f:\n            pickle.dump(test_dataset_rag, f)\n    \n        # Log the file paths to MLFlow for tracking\n        mlflow.log_artifact(train_output_file)\n        mlflow.log_artifact(test_output_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T04:01:01.957215Z","iopub.execute_input":"2024-11-28T04:01:01.957558Z","iopub.status.idle":"2024-11-28T04:01:06.629623Z","shell.execute_reply.started":"2024-11-28T04:01:01.957520Z","shell.execute_reply":"2024-11-28T04:01:06.628725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CREATE RAG PIPELINE","metadata":{}},{"cell_type":"code","source":"from langchain.llms.base import LLM\nfrom typing import Optional\nimport torch\n\nclass ONNXLLM(LLM):\n    model: Optional[object] = None  # Use Optional for model and tokenizer since they're not JSON serializable\n    tokenizer: Optional[object] = None\n    device: Optional[torch.device] = None\n\n    def __init__(self, model, tokenizer, device):\n        super().__init__()\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n\n    @property\n    def _llm_type(self) -> str:\n        return \"onnx_seq2seq\"\n\n    def _call(self, prompt: str, stop: list = None) -> str:\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n        outputs = self.model.generate(\n            inputs[\"input_ids\"],\n            max_length=150,\n            num_beams=5,\n            early_stopping=True,\n            length_penalty=1.0\n        )\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n\nclass BgeRerank(BaseDocumentCompressor):\n    model_name: str = 'BAAI/bge-reranker-v2-m3'\n    top_n: int = 5\n    model: CrossEncoder = CrossEncoder(model_name)\n\n    def bge_rerank(self, query, docs):\n        model_inputs = [[query, doc] for doc in docs]\n        scores = self.model.predict(model_inputs)\n        results = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n        return results[:self.top_n]\n\n    class Config:\n\n        extra = Extra.allow\n        arbitrary_types_allowed = True\n\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks: Optional[Callbacks] = None,\n    ) -> Sequence[Document]:\n        \"\"\"\n        Compress documents using BAAI/bge-reranker models.\n\n        Args:\n            documents: A sequence of documents to compress.\n            query: The query to use for compressing the documents.\n            callbacks: Callbacks to run during the compression process.\n\n        Returns:\n            A sequence of compressed documents.\n        \"\"\"\n        if len(documents) == 0:\n            return []\n        doc_list = list(documents)\n        original_docs = {d.page_content: d for d in doc_list}\n        _docs = [d.page_content for d in doc_list]\n    \n        # Rerank documents based on the query\n        results = self.bge_rerank(query, _docs)\n    \n        # Prepare final results by updating the original documents\n        final_results = []\n        for doc_text, score in results:\n            original_doc = original_docs[doc_text]\n            original_doc.metadata[\"relevance_score\"] = score\n            final_results.append(original_doc)\n        final_results.sort(key=lambda doc: doc.metadata[\"relevance_score\"], reverse=True)\n        return final_results\n\nclass BiomedRobertaEmbeddings(Embeddings):\n        def __init__(self, model, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n            self.model = model.to(device)\n            self.tokenizer = tokenizer\n            self.device = device\n    \n        def embed_query(self, text: str) -> List[float]:\n            tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n            tokens = {key: value.to(self.device) for key, value in tokens.items()}\n            with torch.no_grad():\n                outputs = self.model(**tokens)\n                # Use CLS token representation as embedding\n                return outputs.last_hidden_state[:, 0, :].cpu().numpy().tolist()[0]\n    \n        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n            embeddings = []\n            for text in texts:\n                embeddings.append(self.embed_query(text))\n            return embeddings\n\ndef vectorstore_add_documents_in_batches(vectorstore, documents, batch_size=40000):\n    \"\"\"Add documents to the vectorstore in smaller batches.\"\"\"\n    for i in range(0, len(documents), batch_size):\n        batch = documents[i:i + batch_size]\n        print(f\"Adding documents to vectorstore: batch {i // batch_size + 1}: {len(batch)} documents\")\n        vectorstore.add_documents(batch)\n\ndef format_documents_with_scores(documents: List[Document]) -> str:\n    formatted_docs = [\n        f\"Content: {doc.page_content} \\nRelevance Score: {doc.metadata.get('relevance_score', 'N/A')}\" for doc in documents\n    ]\n    return \"\\n\\n\".join(formatted_docs)\n\ndef create_rag_pipeline(train_dataset_rag):\n    \"\"\"Create a RAGChain pipeline using the fine-tuned model and ChromaDB.\"\"\"\n    mlflow.log_param('device', 'cuda' if torch.cuda.is_available() else 'cpu')\n\n\n    # biomed_model_name = \"allenai/biomed_roberta_base\"\n    # biomed_tokenizer = AutoTokenizer.from_pretrained(biomed_model_name)\n    # biomed_model = AutoModel.from_pretrained(biomed_model_name)\n    \n    # biomed_embeddings = BiomedRobertaEmbeddings(biomed_model, biomed_tokenizer)\n\n    \n    embedding_model = HuggingFaceEmbeddings(\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n        model_kwargs={\"device\": \"cuda:0\"} if torch.cuda.is_available() else {\"device\": \"cpu\"}\n    )\n\n    if os.path.exists(chroma_dir_in):\n        vectorstore = Chroma(persist_directory=chroma_dir_in, embedding_function=embedding_model)\n        print('Vector store loaded from disk')\n    else:\n        vectorstore = Chroma(\n            embedding_function=embedding_model,\n            persist_directory=chroma_dir,\n            collection_name=\"bioasq_train_documents\",\n        )\n        print('Vector store initialized')\n    \n        # vectorstore.add_documents(train_dataset_rag)\n    \n        vectorstore_add_documents_in_batches(vectorstore, train_dataset_rag, batch_size=40000)\n        \n        print('Added documents to Vector store')\n        vectorstore.persist()\n        print('Vector store persisted')\n    \n        mlflow.log_param('rag_vectorstore_description', 'Chroma Vectorstore for BioASQ')\n        mlflow.log_artifact(chroma_dir)\n\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    llm.to(device)\n    langchain_llm = ONNXLLM(model=llm, tokenizer=tokenizer, device=device)\n    \n    print('Creating bm25 retriever')\n    bm25_retriever = BM25Retriever.from_documents(train_dataset_rag)\n    bm25_retriever.k = 15\n    print('bm25 retriever created')\n    vs_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 15})\n    print('vs retriever created')\n\n    ensemble_retriever = EnsembleRetriever(\n        retrievers=[bm25_retriever, vs_retriever], weight=[0.5, 0.5]\n    )\n    print('ensemble retriever created')\n\n    redundant_filter = EmbeddingsRedundantFilter(embeddings=embedding_model)\n    reordering = LongContextReorder()\n    reranker = BgeRerank()\n\n    pipeline_compressor = DocumentCompressorPipeline(\n        transformers=[redundant_filter, reordering, reranker]\n    )\n    print('DocumentCompressorPipeline created')\n\n    compression_pipeline = ContextualCompressionRetriever(\n        base_compressor=pipeline_compressor, base_retriever=ensemble_retriever\n    )\n    print('ContextualCompressionRetriever created')\n\n    runnable_llm = coerce_to_runnable(langchain_llm)\n\n    qa_prompt_template = \"\"\"Answer the following question based on the provided context\n    <context>\n    {context}\n    </context>\n\n    Question:\n    {input}\n    \n    Answer:\"\"\"\n    \n    document_prompt = PromptTemplate(\n        template=\"Context:\\nrelevance score:{relevance_score}\\ncontent:{page_content}\",\n        input_variables=[\"page_content\", \"relevance_score\"],\n    )\n    qa_prompt = PromptTemplate.from_template(qa_prompt_template)\n    # qa_prompt = ChatPromptTemplate.from_template(qa_prompt_template)\n    # qa_prompt = PromptTemplate(\n    #     input_variables=[\"context\", \"input\"],\n    #     template=qa_prompt_template\n    # )\n    # qa_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n    llm_chain = LLMChain(prompt=qa_prompt, llm=runnable_llm, callbacks=None, verbose=True)\n\n    # combine_docs_chain = StuffDocumentsChain(\n    #     llm_chain=llm_chain,\n    #     document_variable_name=\"context\",\n    #     document_prompt=document_prompt,\n    #     callbacks=None,\n    # )\n    \n    # qa_advanced = RetrievalQA.from_chain_type(\n    #     combine_documents_chain=combine_docs_chain,\n    #     callbacks=None,\n    #     verbose=True,\n    #     llm=runnable_llm,\n    #     retriever=compression_pipeline,\n    #     return_source_documents=True,\n    #     chain_type=\"stuff\",\n    #     # chain_type_kwargs = {\n    #     #     \"combine_documents_chain\": combine_docs_chain,\n    #     #     \"verbose\": True\n    #     # },\n    #     # template=prompt\n    # )\n    combine_docs_chain = create_stuff_documents_chain(\n        llm=runnable_llm,\n        prompt=qa_prompt,\n        document_prompt=document_prompt\n    )\n    qa_advanced = create_retrieval_chain(compression_pipeline, combine_docs_chain)\n    print('RAG pipeline created')\n\n    mlflow.log_param('rag_pipeline_created', True)\n    return qa_advanced\n\ndef save_rag_pipeline(rag_chain, rag_chain_path):\n    \"\"\"Save the RAGChain pipeline to a file.\"\"\"\n    rag_chain.llm \n    with open(rag_chain_path, 'wb') as f:\n        pickle.dump(rag_chain, f)\n    print('RAG pipeline saved')\n    mlflow.log_artifact(rag_chain_path)\n\nmlflow.autolog()\nwith mlflow.start_run(run_name=\"RAG Pipeline Creation\", parent_run_id=rag_dataset_creation_run.info.run_id) as rag_pipeline_creation_run:\n    rag_chain = create_rag_pipeline(train_dataset_rag)\n    # save_rag_pipeline(rag_chain, rag_chain_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T04:01:06.630849Z","iopub.execute_input":"2024-11-28T04:01:06.631127Z","iopub.status.idle":"2024-11-28T04:02:27.441246Z","shell.execute_reply.started":"2024-11-28T04:01:06.631101Z","shell.execute_reply":"2024-11-28T04:02:27.440334Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EVALUATE RAG PIPELINE","metadata":{}},{"cell_type":"code","source":"from langsmith import Client\nfrom langsmith.evaluation import LangChainStringEvaluator, evaluate\nimport textwrap\n\ndef get_unique_qa_pairs(test_dataset_rag):\n    seen_qa_pairs = set()\n    unique_qa_pairs = []\n    \n    for item in test_dataset_rag:\n        question = item.metadata['question']\n        answer = item.metadata['ideal_answer']\n        qa_pair = (question, answer)\n        if qa_pair not in seen_qa_pairs:\n            seen_qa_pairs.add(qa_pair)\n            unique_qa_pairs.append({\n                \"question\": question,\n                \"answer\": answer,\n            })\n    print(f\"Filtered {len(test_dataset_rag)} items to {len(unique_qa_pairs)} unique QA pairs.\")\n    return unique_qa_pairs\n\ndef predict_rag_answer(example: dict):\n    \"\"\"Use this for answer evaluation\"\"\"\n    response = rag_chain.invoke({\"input\": example[\"input\"]})\n    return {\"answer\": response['answer']}\n\ndef predict_rag_answer_with_context(example: dict):\n    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n    response = rag_chain.invoke({\"input\": example[\"input\"]})\n    return {\"answer\": response['answer'], \"contexts\": [context.page_content for context in response['context']]}\n\ngrade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n\ndef answer_evaluator(run, example) -> dict:\n    \"\"\"\n    A simple evaluator for RAG answer accuracy\n    \"\"\"\n\n    # Get question, ground truth answer, RAG chain answer\n    input_question = example.inputs[\"input_question\"]\n    reference = example.outputs[\"output_answer\"]\n    prediction = run.outputs[\"answer\"]\n\n    # LLM grader\n    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n    # Structured prompt\n    answer_grader = grade_prompt_answer_accuracy | llm\n\n    # Run evaluator\n    score = answer_grader.invoke({\"question\": input_question,\n                                  \"correct_answer\": reference,\n                                  \"student_answer\": prediction})\n    score = score[\"Score\"]\n\n    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n\ndef evaluate_rag_pipeline(rag_chain, test_dataset_rag, progress_file, batch_size = 20):\n    \"\"\"Evaluate the RAGChain pipeline using the test dataset.\"\"\"\n    print('Evaluating RAG pipeline')\n    client = Client()\n    # client.init_project(\"Bio-ASQ-RAG-LangSmith-eval\")\n    \n    unique_qa_pairs = get_unique_qa_pairs(test_dataset_rag)\n    \n    dataset_name = f\"BioASQ RAG evaluation {uuid.uuid4().hex[:4]}\"\n    dataset = client.create_dataset(\n        dataset_name=dataset_name,\n        description=f\"Test QA for BioASQ RAG\",\n    )\n    random_50_qa_pairs = random.sample(unique_qa_pairs, 50)\n    inputs, outputs = zip(\n        *[({\"input\": qa[\"question\"]}, {\"expected\": qa[\"answer\"]}) for qa in random_50_qa_pairs]\n    )\n    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n    \n    # qa_evalulator = [\n    #     LangChainStringEvaluator(\n    #         \"cot_qa\",\n    #         prepare_data=lambda run, example: (\n    #             {\n    #                 \"prediction\": run.outputs[\"answer\"],\n    #                 \"reference\": example.outputs[\"expected\"],\n    #                 \"input\": example.inputs[\"input\"],\n    #             }\n    #         ),\n    #     )\n    # ]\n    # qa_evalulator_experiment_results = evaluate(\n    #     predict_rag_answer,\n    #     data=dataset_name,\n    #     evaluators=qa_evalulator,\n    #     experiment_prefix=f\"rag-bioasq-oai\",\n    #     metadata={\"variant\": \"LCEL context, gpt-4o-mini\"},\n    # )\n    \n    answer_hallucination_evaluator = LangChainStringEvaluator(\n        \"labeled_score_string\",\n        config={\n            \"criteria\": {\n                \"accuracy\": \"\"\"Is the Assistant's Answer grounded in the Ground Truth documentation? A score of [[1]] means that the\n                Assistant answer contains is not at all based upon / grounded in the Groun Truth documentation. A score of [[5]] means \n                that the Assistant answer contains some information (e.g., a hallucination) that is not captured in the Ground Truth \n                documentation. A score of [[10]] means that the Assistant answer is fully based upon the in the Ground Truth documentation.\"\"\"\n            },\n            # If you want the score to be saved on a scale from 0 to 1\n            \"normalize_by\": 10,\n        },\n        prepare_data=lambda run, example: {\n            \"prediction\": run.outputs[\"answer\"],\n            \"reference\": run.outputs[\"contexts\"],\n            \"input\": example.inputs[\"input\"],\n        },\n    )\n    answer_hallucination_evaluator_experiment_results = evaluate(\n        predict_rag_answer_with_context,\n        data=dataset_name,\n        evaluators=[answer_hallucination_evaluator],\n        experiment_prefix=f\"rag-bioasq-oai-hallucination\",\n        # Any experiment metadata can be specified here\n        metadata={\n            \"variant\": \"LCEL context, gpt-4o-mini\",\n        },\n    )\n\n    docs_relevance_evaluator = LangChainStringEvaluator(\n        \"score_string\",\n        config={\n            \"criteria\": {\n                \"document_relevance\": textwrap.dedent(\n                    \"\"\"The response is a set of documents retrieved from a vectorstore. The input is a question\n                used for retrieval. You will score whether the Assistant's response (retrieved docs) is relevant to the Ground Truth \n                question. A score of [[1]] means that none of the  Assistant's response documents contain information useful in answering or addressing the user's input.\n                A score of [[5]] means that the Assistant answer contains some relevant documents that can at least partially answer the user's question or input. \n                A score of [[10]] means that the user input can be fully answered using the content in the first retrieved doc(s).\"\"\"\n                )\n            },\n            # If you want the score to be saved on a scale from 0 to 1\n            \"normalize_by\": 10,\n        },\n        prepare_data=lambda run, example: {\n            \"prediction\": run.outputs[\"contexts\"],\n            \"input\": example.inputs[\"input\"],\n        },\n    )\n    docs_relevance_evaluator_experiment_results = evaluate(\n        predict_rag_answer_with_context,\n        data=dataset_name,\n        evaluators=[docs_relevance_evaluator],\n        experiment_prefix=f\"rag-bioasq-oai-doc-relevance\",\n        # Any experiment metadata can be specified here\n        metadata={\n            \"variant\": \"LCEL context, gpt-4o-mini\",\n        },\n    )      \n\n        \n\nmlflow.autolog()\nwith mlflow.start_run(run_name=\"RAG Pipeline Evaluation\", parent_run_id=rag_pipeline_creation_run.info.run_id) as rag_pipeline_evaluation_run:\n    progress_file = '/kaggle/input/bioasq-rag-evaluation-progress/eval_progress_2.json'\n    eval_results = evaluate_rag_pipeline(rag_chain, test_dataset_rag, progress_file)\n    # mlflow.log_metric('rag_pipeline_evaluation', eval_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T04:02:27.443986Z","iopub.execute_input":"2024-11-28T04:02:27.444260Z"}},"outputs":[],"execution_count":null}]}